{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k3larra/generalisation/blob/pc/shapes_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hTdz9j-fzHH"
      },
      "source": [
        "# Training model on basic 2D shapes\n",
        "This code accompanies the paper with the title \"Deep Learning, generalisation and concepts\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "Iwwa9AA2j9Rg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJ1mX07G3pSU",
        "outputId": "1860bed0-3192-4210-a9b1-e9a772c9f1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.13.0+cu116\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import uuid\n",
        "import shutil\n",
        "from os import listdir, mkdir\n",
        "from os.path import isfile, join\n",
        "import json\n",
        "import random\n",
        "random.seed(131254) # Set seed for reproducibility.\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision.io import read_image\n",
        "from torchvision.io import ImageReadMode\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as transformsF\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from zipfile import ZipFile\n",
        "print(torch.__version__)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!mkdir '/content/study'\n",
        "!wget https://raw.githubusercontent.com/k3larra/generalisation/pc/models/models.json -P /content/study"
      ],
      "metadata": {
        "id": "a_0VJZ9eyAwa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Cleaning if needed\n",
        "# ! rm -r study\n",
        "# ! rm -r shapes\n",
        "# ! rm classes.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wFdXawpraLC",
        "outputId": "1525dece-7bf6-4c93-9532-546b3b032a86"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'classes.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training set"
      ],
      "metadata": {
        "id": "CmP9NmdrRMot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcvX2E8pVLhP"
      },
      "outputs": [],
      "source": [
        "#The kaggle token\n",
        "#!pip install kaggle\n",
        "!mkdir ~/.kaggle\n",
        "!touch ~/.kaggle/kaggle.json\n",
        "api_token = {\"username\":\"your_username\",\"key\":\"your_key\"}\n",
        "with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(api_token, file)\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download smeschke/four-shapes\n",
        "with ZipFile('/content/four-shapes.zip', 'r') as archive:\n",
        "  archive.extractall('/content')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRw7FfUxQxqT"
      },
      "source": [
        "## Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rS18X7Y_Grm5"
      },
      "outputs": [],
      "source": [
        "# Create annotation file for training!\n",
        "shape_path = \"/content/shapes\"\n",
        "shapes_to_ignore = ['0.png', '1.png', '2.png', '3.png', '4.png', '5.png', '6.png', '7.png', '8.png', '9.png']\n",
        "labels_to_ignore = []\n",
        "with open('/content/shapes/shapes.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow(['image_location', 'label']) # Write header\n",
        "    for dirs in listdir(\"/content/shapes\"):\n",
        "        if dirs in labels_to_ignore: #Ignore label!\n",
        "          continue\n",
        "#         print(dirs)\n",
        "        if(dirs.endswith(\".csv\")):\n",
        "            continue\n",
        "        for f in listdir(join(\"/content/shapes\", dirs)):\n",
        "            if f in shapes_to_ignore: #ignore the images! \n",
        "              continue\n",
        "            elif(join(\"/content/shapes\", dirs, f).endswith(\".csv\")):\n",
        "                continue\n",
        "            elif isfile(join(\"/content/shapes\", dirs, f)):\n",
        "                writer.writerow([join(dirs, f), dirs])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# Create classes\n",
        "!wget https://raw.githubusercontent.com/k3larra/generalisation/pc/classes.txt -P /content\n",
        "with open(\"classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "num_classes = len(categories)\n",
        "\n",
        "def label_to_idx(label):\n",
        "  return categories.index(label)\n",
        "\n",
        "def idx_to_label(idx):\n",
        "  return categories[idx]"
      ],
      "metadata": {
        "id": "e1MXBAZAmpU_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "M0uiQeWtF77u"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "        self.resizer = transforms.Resize(size=(299, 299)) \n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        #image = read_image(img_path,ImageReadMode.RGB) \n",
        "        image = read_image(img_path,ImageReadMode.UNCHANGED) \n",
        "        image = image.float()\n",
        "        image /= 255. \n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        label = torch.tensor(label_to_idx(label))\n",
        "        # ADDDITIONS TO TRANSFORM THE INPUT\n",
        "        r = random.random() # 33% (equal probability) to zoom-in, zoom-out, dont transform\n",
        "        if r < 0.33: #zoom-in the shape!\n",
        "          top = random.randrange(0, 40) \n",
        "          left = top\n",
        "          dif = 40 - top\n",
        "          w = 120 + dif\n",
        "          h = w\n",
        "          image = transformsF.resized_crop(image, top, left, h, w, size=(200, 200))\n",
        "        elif r < 0.66: #zoom-out the shape!\n",
        "          pad = random.randrange(0, 300)\n",
        "          image = transforms.Pad(padding=pad, fill=1)(image)\n",
        "        image = self.resizer(image) #Perhaps either native or this? I guess 299,299 is ok since we do 224x 224 later if ResNet but effisientet use [384,384], inception [299,299], but then our shape images are 200,200\n",
        "        # END OF ADDITIONS!\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        image = image.repeat(3, 1, 1)# make RGB\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and validation set"
      ],
      "metadata": {
        "id": "RzK1v3DmOiAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Models to train\n",
        "# from torchvision.models import resnet50, ResNet50_Weights\n",
        "# #model_resnet50 = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
        "# model_resnet50 = resnet50(weights=None)\n",
        "# # model_resnet50.weights = \"ResNet50_Weights.IMAGENET1K_V2\"\n",
        "# model_resnet50._weights = \"None\" ##For the JSON\n",
        "# model_resnet50._name = \"ResNet50\" ##For the JSON\n",
        "# model_resnet50._transforms = \"transforms.Resize(size=(299, 299))\"\n",
        "# model_resnet50.fc = nn.Linear(model_resnet50.fc.in_features, num_classes)\n",
        "\n",
        "#training_loader,validation_loader = createTraining_and_validation_loader(transform=ResNet50_Weights.IMAGENET1K_V2.transforms())\n",
        "\n",
        "# from torchvision.models import resnet101, ResNet101_Weights\n",
        "# model_resnet101 = resnet101(weights=ResNet101_Weights.IMAGENET1K_V2)\n",
        "# model_resnet101.name = \"ResNet101\"\n",
        "\n",
        "# 152 seems to be to big for standard colab and the batch size used\n",
        "# from torchvision.models import resnet152, ResNet152_Weights\n",
        "# model_resnet152 = resnet152(weights=ResNet152_Weights.IMAGENET1K_V2)\n",
        "# model_resnet152.name = \"ResNet152\"\n",
        "\n",
        "# from torchvision.models import googlenet, GoogLeNet_Weights\n",
        "# model_googlenet = googlenet(weights=GoogLeNet_Weights.IMAGENET1K_V1)\n",
        "# model_googlenet.name = \"GoogLeNet\"\n",
        "\n",
        "# from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "# model_inception_v3 = inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1)\n",
        "# model_inception_v3.name = \"Inception_V3\"\n",
        "from torchvision.models import inception_v3\n",
        "model_inception_v3 = inception_v3(weights=None)\n",
        "model_inception_v3.aux_logits=False #needed for inception ??\n",
        "model_inception_v3._name = \"Inception_V3\"\n",
        "model_inception_v3._weights = \"None\" ##For the JSON\n",
        "model_inception_v3._transforms = \"transforms.Resize(size=(299, 299))\"\n",
        "model_inception_v3.fc = nn.Linear(model_inception_v3.fc.in_features, num_classes)\n",
        "\n",
        "# from torchvision.models import convnext_tiny,ConvNeXt_Tiny_Weights\n",
        "# model_convnext_tiny = convnext_tiny(weights=ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "# model_convnext_tiny.name = \"ConvNeXt_Tiny\"\n",
        "\n",
        "experiment_models = [model_inception_v3]\n",
        "#experiment_weights= [ResNet50_Weights.IMAGENET1K_V2]\n",
        "\n",
        "# experiment_models = [model_resnet50,   \n",
        "#                      model_resnet152,\n",
        "#                      model_googlenet,\n",
        "#                      model_inception_v3,\n",
        "#                      model_convnext_tiny]\n",
        "# experiment_weights= [ResNet50_Weights.IMAGENET1K_V2,\n",
        "#                      ResNet152_Weights.IMAGENET1K_V2,\n",
        "#                      GoogLeNet_Weights.IMAGENET1K_V1,\n",
        "#                      Inception_V3_Weights.IMAGENET1K_V1,\n",
        "#                      ConvNeXt_Tiny_Weights.IMAGENET1K_V1]"
      ],
      "metadata": {
        "id": "WBeE_JtGvYKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#transform=ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "#shapes_dataset = CustomImageDataset('/content/shapes/shapes.csv', '/content/shapes',transform=transform, target_transform=None)\n",
        "shapes_dataset = CustomImageDataset('/content/shapes/shapes.csv', '/content/shapes',transform=None, target_transform=None)\n",
        "input, label = shapes_dataset[0]\n",
        "training_size = int(len(shapes_dataset)*0.9)\n",
        "validation_size = int(len(shapes_dataset) - training_size)\n",
        "train_set, val_set = torch.utils.data.random_split(shapes_dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(42)) #Use seed for reproducability\n",
        "training_loader = DataLoader(train_set, batch_size=32, shuffle=True)  #32,64\n",
        "validation_loader = DataLoader(val_set, batch_size=32, shuffle=True)  #32,64"
      ],
      "metadata": {
        "id": "zmLPLkpYIqQC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6NRPbZ8p8p"
      },
      "source": [
        "# Training set information "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S66ZZUxiN5wb"
      },
      "outputs": [],
      "source": [
        "#transform=ResNet50_Weights.IMAGENET1K_V2.transforms()\n",
        "#shapes_dataset = CustomImageDataset('/content/shapes/shapes.csv', '/content/shapes',transform=transform, target_transform=None)\n",
        "shapes_dataset = CustomImageDataset('/content/shapes/shapes.csv', '/content/shapes',transform=None, target_transform=None)\n",
        "# I find it strange that using the standard ResNet transform creates images that are not [0..1] or [0..255]  \n",
        "figure = plt.figure(figsize=(12, 4))\n",
        "cols, rows = 6, 2\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(shapes_dataset), size=(1,)).item() # Get rnd_id\n",
        "    sample = shapes_dataset[sample_idx] # index dataset (gets you the img and label - index)\n",
        "    img, label = shapes_dataset[sample_idx] # index dataset (gets you the img and label - index)\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    img = img.permute(1,2,0)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img) # put channel as the last dimension out, vmin=0, vmax=255\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, labels in training_loader:\n",
        "  print(inputs.shape)\n",
        "  print('Training set size',int(len(inputs)))\n",
        "  #print(inputs)\n",
        "  print(labels.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "Scyvu5mweyC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and save models"
      ],
      "metadata": {
        "id": "Acil7fv1Y-nP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "9l0Co_PqRVol"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer):\n",
        "    jsonData={}\n",
        "    best_acc = 0.0\n",
        "    model.train()   # Set model to train mode\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    for inputs, labels in training_loader:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        loss = criterion(outputs, labels)    \n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()     \n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)     \n",
        "    epoch_loss = running_loss / training_size\n",
        "    epoch_acc = running_corrects.double() / training_size\n",
        "    jsonData[\"train_loss\"] = round(epoch_loss,3)\n",
        "    jsonData[\"epoch_acc\"] = round(epoch_acc.item(),3)\n",
        "    print('train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "    return jsonData\n",
        "    \n",
        "def test_model(model, criterion):\n",
        "  jsonData={}\n",
        "  best_acc = 0.0\n",
        "  model.eval()   # Set model to evaluate mode\n",
        "  running_loss = 0.0\n",
        "  running_corrects = 0\n",
        "  for inputs, labels in validation_loader:\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      loss = criterion(outputs, labels)\n",
        "      running_loss += loss.item() * inputs.size(0)\n",
        "      running_corrects += torch.sum(preds == labels.data)\n",
        "  epoch_loss = running_loss / validation_size\n",
        "  epoch_acc = running_corrects.double() / validation_size\n",
        "  jsonData[\"test_loss\"] = round(epoch_loss,3)\n",
        "  jsonData[\"epoch_acc\"] = round(epoch_acc.item(),3)\n",
        "  print('test Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "  return jsonData"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i3zDqZ0QRlhj"
      },
      "outputs": [],
      "source": [
        "#Run training and add information to the JSON file\n",
        "model_save_path = '/content/study/'\n",
        "json_filename = 'models.json'\n",
        "experiment_model=None\n",
        "num_epochs = 7\n",
        "if isfile(model_save_path + json_filename) is True:\n",
        "  with open(model_save_path + json_filename) as outfile:\n",
        "    experiment_json_data = json.load(outfile)\n",
        "else:\n",
        "  experiment_json_data={}\n",
        "for experiment_model in experiment_models:\n",
        "  jsonData={}\n",
        "  id = str(uuid.uuid4())\n",
        "  complete_model_path = id+'_complete_model.pth' \n",
        "  jsonData[\"complete_model_path\"] = complete_model_path\n",
        "  jsonData[\"model_name\"] = experiment_model._name\n",
        "  jsonData[\"weights\"]= experiment_model._weights\n",
        "  jsonData[\"num_epocs\"] = num_epochs\n",
        "  jsonData[\"transformation\"] = experiment_model._transforms\n",
        "  jsonData[\"time\"] = time.asctime( time.localtime(time.time()))\n",
        "  print('--')\n",
        "  #model_ft = experiment_model\n",
        "  experiment_model.eval()\n",
        "  experiment_model = experiment_model.to(device)\n",
        " \n",
        "  print(\"Training:\",experiment_model._name)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer_ft = optim.SGD(experiment_model.parameters(), lr=0.001, momentum=0.9)\n",
        "  scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "  jsonTrain={}\n",
        "  train=0\n",
        "  test = 0\n",
        "  for epoch in range(num_epochs):\n",
        "      print()\n",
        "      print('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
        "      jsonTrain[\"train_\"+str(epoch)] = train_model(experiment_model, criterion, optimizer_ft)\n",
        "      jsonTrain[\"test_\"+str(epoch)] = test_model(experiment_model, criterion)\n",
        "      scheduler.step()\n",
        "  torch.save(experiment_model, model_save_path+complete_model_path)\n",
        "  jsonData[\"training\"] = jsonTrain\n",
        "  experiment_json_data[id] = jsonData\n",
        "with open(model_save_path + json_filename, 'w') as outfile:\n",
        "  outfile.write(json.dumps(experiment_json_data))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "zip_name = \"trained_models\"\n",
        "shutil.make_archive(zip_name, 'zip', \"study\")"
      ],
      "metadata": {
        "id": "qEOdfKADJR4U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}